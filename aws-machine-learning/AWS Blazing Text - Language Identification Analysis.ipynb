{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "my_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_name = 'sg-ml-bucket-02'       # Change to your bucket\n",
    "prefix = 'language_identification/fasttext' # and prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blazing_text_container = sagemaker.amazon.amazon_estimator.get_image_uri(my_region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(blazing_text_container, my_region_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O model.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tar the Language Identification and load to S3 bucket \n",
    "!tar -czvf langid.tar.gz model.bin\n",
    "blazing_text_model_location = my_session.upload_data(\"langid.tar.gz\", bucket=bucket_name, key_prefix=prefix)\n",
    "!rm langid.tar.gz model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup inference endpoint for model inference analysis\n",
    "language_identifier = sagemaker.Model(model_data=blazing_text_model_location, image_uri=blazing_text_container, role=role, sagemaker_session=my_session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_identifier.deploy(initial_instance_count = 1,instance_type = 'ml.m5.xlarge')\n",
    "\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "language_identifier_predictor = sagemaker.Predictor(endpoint_name=language_identifier.endpoint_name, \n",
    "                                   sagemaker_session=my_session,\n",
    "                                   serializer=JSONSerializer(), #json.dumps,\n",
    "                                   deserializer=JSONDeserializer()) #sagemaker.predictor.json_deserializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_language_examples = [\"À quoi sert l'intelligence artificielle\",\n",
    "             \"Was ist der Zweck der künstlichen Intelligenz?\",\n",
    "             \"Wat is die doel van kunsmatige intelligensie\",\n",
    "             \"ما هو الغرض من الذكاء الاصطناعي\",\n",
    "             \"Süni intellektin məqsədi nədir\",\n",
    "             \"Hvad er formålet med kunstig intelligens\"]\n",
    "prediction_input = {\"instances\" : some_language_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_predictions = language_identifier_predictor.predict(prediction_input)\n",
    "print(language_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the '__label__' before each language identifier in the prediction output\n",
    "# and change the label and prob to more readable values\n",
    "for output in language_predictions:\n",
    "    output['label'] = output['label'][0][9:].upper() # remove __label__ preceding the language identifier\n",
    "    output['language'] = output.pop('label')         # make the labels \n",
    "    output['probability'] = output.pop('prob')       # readable\n",
    "\n",
    "print(language_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
